---
title: "[Paper Review] The Llama 3 Herd of Models"
date: 2025-1-20
author: jieun
math: True
categories: [Language-Model]
tags: [LLM, sLLM, Llama]
typora-root-url: ..
---

[Llama 2](https://jieun121070.github.io/posts/LLaMA2/)의 후속 모델인 Llama 3는 Llama 2와 전체적으로 비슷한 모델 구조를 갖고 있습니다. 본 포스트에서는 Llama 2와 비교해서 어떤 변화가 있었고, 결과적으로 어느 정도의 성능 향상이 있었는지를 중점적으로 다뤄보겠습니다.

## 학습 데이터

Llama 3에서는 Llama 2보다 데이터의 양과 질을 크게 향상시켜 사용했는데요. Llama 2에서 pre-training에 약 1.8조 개의 토큰을 사용한 것과 비교해, Llama 3에서는 약 **15조 개의 다국어 토큰**을 사용했습니다. 토큰 개수를 8개 가량 증가시킨 것입니다. 또한, 섬세한 전처리 과정을 거친 데이터를 pre-training에 사용하고, 엄격한 기준으로 필터링한 데이터로 post-training을 진행했습니다. 

## 모델 버전별 차이

![](/assets/img/llm/llama3_model_scale.png)

Llama 3 시리즈는 크게 초기 모델에 속하는 Llama 3와 Llama 3.1 이후로 나누어 볼 수 있습니다. Llama 3는 8B, 70B 두 가지 크기로 출시되었습니다. 다국어 데이터로 pre-training을 거친 모델이긴 하지만, 영어 최적화를 목표로 개발되어서 비영어권 언어 성능이 다소 떨어집니다. 그리고 context window가 8K로 다소 짧은 수준이고, tool 사용이 불가능하다는 한계점이 있습니다. 여기에서 tool 사용이란 LLM이 내부 지식만으로 해결할 수 없는 작업을 외부의 특정 도구를 호출하여 해결하는 능력을 말합니다. 예를 들어, "내일 서울 날씨는 어때?"와 같은 질문에 답하기 위해 실시간 날씨 정보를 얻을 수 있는 날씨 API를 호출하는 것입니다.

후속 모델인 Llama 3.1부터는 **다국어 성능이 향상**되고, **context window**가 **128K**로 크게 증가했습니다. 또한 **tool 사용도 가능**해 졌습니다.

## 모델 구조

### Pre-Training

앞서 설명한 것처럼, Llama 3와 Llama 2는 pre-training 구조 차체는 동일합니다. Llama 시리즈는 Llama 1부터 Llama 2, 그리고 Llama 3에 이르기까지 다음과 같은 핵심적인 구조를 공유하고 있습니다.

- `Pre-normalization`: Transformer block의 각 sub-layer에 들어가는 input을 정규화하는 구조입니다.
- `RMSNorm`: Pre-normalization에 사용되는 정규화 알고리즘으로, 계산 효율성이 뛰어납니다.
- `SwiGLU`: 기존의 ReLU보다 더 복잡하고 유연한 표현이 가능한 activation function입니다.
- `Rotary Positional Embeddings (RoPE)`: 토큰의 위치에 따라 embedding vector를 일정 각도로 회전시켜서 순서 정보를 입력하는 방식으로, 긴 시퀀스 처리에 유리합니다.

### Post-Training

![](/assets/img/llm/llama3_post_training.png)

## Reference

- [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/)

