---
title: "LoRA: Low-Rank Adaptation of Large Language Models"
date: 2024-3-15
author: jieun
math: True
categories: [Language-Model]
tags: [LLM, LoRA]
typora-root-url: ..
---

**LoRA(2021)**는 LLM을 효율적으로 fine-tuning하기 위해 제안된 모델입니다. 기존 weight는 그대로 두고, 저차원 보조 행렬만 학습해 메모리를 크게 줄이면서도 전체 fine-tuning과 동등한 성능을 달성했습니다.

## 모델 구조

![](/assets/img/llm/lora.png)

1. 랭크 $r$ 선택 – 보통 4~16
2. 각 Transformer weight $W$ ($d \times d$) 옆에 **$A$ ($r \times d$), $B$ ($d \times r$)** 두 행렬을 삽입
3. 학습 중엔 **$A$, $B$**만 업데이트 → 연산량 증가 $\le$ 1%
4. 추론 시엔 $W + A \cdot B$를 미리 합쳐 두어 **추가 지연 발생하지 않음**