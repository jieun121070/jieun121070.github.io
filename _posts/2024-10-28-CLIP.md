---
title: "[Paper Review] CLIP: Learning Transferable Visual Models From Natural Language Supervision"
date: 2024-10-28
author: jieun
math: True
categories: [Multimodal]
tags: [CLIP]
typora-root-url: ..
---











OpenAI가 2020년에 발표한 [GPT-3](https://jieun121070.github.io/posts/GPT3/)는 1750억 개의 파라미터를 가진 거대 decoder-only transformer 모델입니다. GPT-3의 연구진은 방대한 양의 데이터셋을 causal LM 만으로 학습해도 다양한 task에 범용적으로 활용할 수 있음을 입증했습니다. GPT-3 등장 이후 LLM 연구는 task별로 특화된 모델을 만들기보다는, 하나의 거대한 범용 모델을 만드는 방향으로 전환되었습니다. 이러한 변화를 VIsion 분야에 적용하고자 한 연구가 오늘 소개할 **[CLIP](https://arxiv.org/pdf/2103.00020)** 입니다.

CLIP은 OpenAI에서 발표한 멀티모달 모델입니다. 텍스트와 이미지를 같이 학습해서, 둘을 같은 embedding space에서 비교하거나 연결할 수 있도록 만들었습니다. 전통적인 이미지 분류 모델은 고정된 클래스를 예측하도록 학습되었지만, CLIP은 자연어를 통해 새로운 클래스를 유연하게 정의하고, 분류할 수 있는 특징이 있습니다.

## 1. 모델 구조

![](/assets/img/diffusion/clip_pretrain.png)