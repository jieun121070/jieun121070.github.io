---
title: "[Paper Review] DALL-E 2: Hierarchical Text-Conditional
Image Generation with CLIP Latents"
date: 2024-12-9
author: jieun
math: True
categories: [Multimodal]
tags: [DALL-E, CLIP, Diffusion]
typora-root-url: ..
---

DALL-E 2는 [DALL-E](https://jieun121070.github.io/posts/DALL-E/)의 후속 모델로, 텍스트 설명을 바탕으로 이미지를 생성하는 text-to-image 모델입니다. 선행 연구들은 주로 텍스트로부터 이미지를 바로 생성했습니다. 그런데 텍스트가 이미지를 100% 완벽하게 설명하는 경우는 거의 없기 때문에, 이런 방식으로 생성한 이미지의 품질에는 한계가 있을 수밖에 없습니다.

이와 달리 DALL-E 2는 이미지 생성 과정을 2단계로 나누어 **텍스트 설명(caption)을 입력받아 이미지 embedding을 생성한 다음, 이미지 embedding을 조건으로 이미지를 생성**합니다. 조건부 분포 $p(\text{image\_embedding} \vert \text{text\_caption})$를 명시적으로 모델링한 다음 $p(\text{image} \vert \text{image\_embedding})$을 모델링하는 것입니다. 이렇게 명시적으로 이미지 embedding을 생성하는 것이 이미지의 다양성을 향상시키고,  이미지 embedding을 생성하는 데는 [**CLIP**](https://jieun121070.github.io/posts/CLIP/) 모델을 사용하고, 이미지를 생성하는 데는 **Diffusion** 모델을 사용했습니다. 이제 DALL-E 2의 모델 구조, 학습 방법과 성능에 대해 좀 더 자세히 알아보겠습니다. 

## 모델 학습

![](/assets/img/diffusion/dalle-2.png)

_DALL-E 2 architecture_

앞서 설명한 것처럼 DALL-E 2는 CLIP과 Diffusion 모델을 함께 사용하는 모델입니다. 모델 학습과 이미지 생성 과정에서 CLIP과 Diffusion 모델이 어떻게 사용되는지 자세히 살펴보겠습니다.

### CLIP 모델 pre-training

DALL-E 2의 학습에 앞서, 대규모의 (텍스트, 이미지) 쌍 데이터셋을 사용해 CLIP을 pre-training합니다. CLIP의 학습 목표는 이미지와 텍스트를 동일한 다차원 공간에 매핑하여, 서로 관련된 이미지와 텍스트의 embedding은 가깝게, 관련 없는 embedding들은 멀게 위치하도록 만드는 것입니다. 그 결과로 텍스트와 이미지를 embedding으로 변환하는 **텍스트 인코더**와 **이미지 인코더**가 완성됩니다.

DALL-E 2는 모델 학습의 첫 번째 단계에서 이 인코더들을 가져와 사용하는데, 위 이미지에서 `text encoder`와 `img encoder` 부분에 해당합니다. (텍스트, 이미지) 쌍으로 이루어진 데이터셋에서 텍스트와 이미지를 각각 텍스트 인코더와 이미지 인코더에 입력하여 embedding $z_t$와 $z_i$를 얻습니다.

### Prior 학습

CLIP으로 이미지와 텍스트의 embedding을 얻은 이후, DALL-E 2의 학습은 두 가지의 단계 prior와 decoder로 나누어 진행됩니다. 먼저 prior $p(z_i \vert y)$는 텍스트 $y$를 입력받아 CLIP 이미지 embedding $z_i$를 생성하는 모델입니다. 저자들은 두 가지의 prior 모델 Autoregressive(AR) prior와 Diffusion prior를 모두 실험했는데요. 결과적으로 **Diffusion prior가 더 좋은 성능**을 보였다고 합니다.

**Autoregressive(AR) prior**



**Diffusion prior**

노이즈가 추가된 $z_i$로부터 원본 $z_i$를 복원하는 데 필요한 노이즈를 예측하도록 학습됩니다.

### Decoder 학습

decoder $p(x \vert z_i, y)$는 prior 모델에서 예측한 CLIP 이미지 embedding $z_i$를 입력으로 받아 실제 이미지 $x$를 생성하는 diffusion 모델입니다. 텍스트 $y$는 decoder의 성능을 높이기 위해 선택적으로 사용될 수 있습니다.

decoder의 입력 값을 구체적으로 살펴보면, (CLIP 이미지 embedding $z_i$, CLIP 텍스트 embedding $z_t$, 실제 픽셀 이미지)의 삼중항으로 구성됩니다. 이 데이터는 원본 (텍스트, 이미지) 쌍으로부터 $z_i$와 $z_t$를 생성하여 구성됩니다.

디코더는 CLIP 이미지 임베딩 $z_i$과 텍스트 임베딩 $z_t$을 조건으로 받아, 실제 픽셀 이미지 $x_0$를 정확하게 생성하는 방법을 학습합니다. diffusion model로, 노이즈가 추가된 픽셀 이미지로부터 원본 픽셀 이미지를 복원하는 데 필요한 노이즈를 예측하도록 훈련됩니다.

prior와 decoder를 쌓으면 텍스트 $y$가 주어졌을 때 이미지 $x$를 생성하는 생성 모델 $p(x \vert y)$가 만들어집니다.

$$P(x \vert y)=P(x \vert z_i,y)=P(x \vert z_i,y)P(z_i \vert y)$$

## DDIM

DDPM은 diffusion model의 역확산 과정을 이산적인 마르코프 체인으로 정의했기 때문에 순방향 확산 과정과 동일한 스텝만큼 샘플링을 거쳐야 고품질의 이미지 생성이 가능하다. 

하지만 DDIM은 DDPM의 역확산 과정에서 확률적 노이즈 항을 제거하여 역확산 경로를 결정론적으로 계산할 수 있도록 바꾸었다. 이에 따라 역확산 과정을 연속적인 ODE(Ordinary Differential Equation)의 이산적인 근사로 해석할 수 있게 되었고, 마르코프 속성에 얽매이지 않고 스킵 샘플링을 할 수 있게 되었다. 샘플링 스텝 수가 획기적으로 감소한 것이다.

## Reference

- [But how do AI videos actually work?](https://www.youtube.com/watch?v=iv-5mZ_9CPY)