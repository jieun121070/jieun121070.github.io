---
title: "[Paper Review] DALL-E 2: Hierarchical Text-Conditional
Image Generation with CLIP Latents"
date: 2024-12-9
author: jieun
math: True
categories: [Multimodal]
tags: [DALL-E, CLIP, Diffusion]
typora-root-url: ..
---

DALL-E 2는 [DALL-E](https://jieun121070.github.io/posts/DALL-E/)의 후속 모델로, 텍스트 설명을 바탕으로 이미지를 생성하는 text-to-image 모델입니다. 선행 연구들은 주로 텍스트로부터 이미지를 바로 생성했습니다. 그런데 텍스트가 이미지를 100% 완벽하게 설명하는 경우는 거의 없기 때문에, 이런 방식으로 생성한 이미지의 품질에는 한계가 있을 수밖에 없습니다.

이와 달리 DALL-E 2는 이미지 생성 과정을 2단계로 나누어 **텍스트 설명(caption)을 입력받아 이미지 embedding을 생성한 다음, 이미지 embedding을 조건으로 이미지를 생성**합니다. 조건부 분포 $p(\text{image_embedding}|\text{text_caption})$를 명시적으로 모델링한 다음 $p(\text{image}|\text{image_embedding})$을 모델링하는 것입니다. 이렇게 명시적으로 이미지 embedding을 생성하는 것이 이미지의 다양성을 향상시키고,  이미지 embedding을 생성하는 데는 [CLIP](https://jieun121070.github.io/posts/CLIP/) 모델을 사용하고, 이미지를 생성하는 데는 Diffusion 모델을 사용했습니다. 이제 DALL-E 2의 모델 구조, 학습 방법과 성능에 대해 좀 더 자세히 알아보겠습니다. 

## 모델 구조

![](/assets/img/diffusion/dalle-2.png)

_DALL-E 2 architecture_

앞서 설명한 것처럼 DALL-E 2는 CLIP과 Diffusion 모델을 함께 사용하는 모델입니다.

