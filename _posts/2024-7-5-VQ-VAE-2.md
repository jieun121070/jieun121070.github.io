---
title: "[Paper Review] Generating Diverse High-Fidelity Images with VQ-VAE-2"
date: 2024-7-5
author: jieun
math: True
categories: [Vision]
tags: [VAE, VQ-VAE]
typora-root-url: ..
---

[VQ-VAE 2](https://arxiv.org/pdf/1906.00446)는 [VQ-VAE](https://jieun121070.github.io/posts/VQ-VAE-Vector-Quantised-Variational-AutoEncoder/)의 한계점을 개선하기 위해 나온 후속 모델입니다. 저자들은 [이전 논문]()에서 아래와 같이 언급했는데요.

> Training the prior and the VQ-VAE jointly, which could strengthen our results, is left as future research.

여기서 향후 과제로 남겨둔, **VQ-VAE와 prior를 함께 학습**하는 모델이 바로 VQ-VAE 2입니다. 또한 **prior $p(z)$를 학습하는 구조를 계층적으로 변경**하여 이미지 생성 품질을 한 층 끌어올렸습니다. VQ-VAE 2에 대해 자세히 알아보기 전에 VQ-VAE와의 차이점을 간단히 정리해보면 아래와 같습니다.

|               | VQ-VAE (2017)                                                | VQ-VAE-2 (2019)                                              |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Latent Layers | Single latent layer                                          | Multi-scale hierarchical latent layers                       |
| prior $p(z)$  | single-layer autoregressive prior<br />VQ-VAE 학습이 끝난 뒤 따로 학습 | hierarchical autoregressive priors for each latent level<br />VQ-VAE 2와 함께 학습 |

## 1. 모델 구조

![](/assets/img/diffusion/vqvae2.png)

VQ-VAE 2는 이미지를 pixel space가 아니라 descrete latent space로 매핑한다는 점에서는 VQ-VAE와 동일합니다. 하지만 latent space와 prior를 계층적으로 변경했다는 점에서 차이가 있습니다.

## 2. 모델 학습

![](/assets/img/diffusion/vqvae_train.png)