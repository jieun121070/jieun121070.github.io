---
title: "[Paper Review] Qwen 2.5"
date: 2025-2-4
author: jieun
math: True
categories: [Language-Model]
tags: [LLM, Qwen]
typora-root-url: ..
---

Qwen은 알리바바 클라우드에서 개발한 LLM으로, 코딩과 수학적 추론 능력이 뛰어난 모델입니다.

## 1. 학습 데이터

Pre-training 데이터셋을 기존(Qwen 2) 7조 토큰에서 18조 토큰으로 늘렸습니다.

![](/assets/img/llm/qwen1.png)

## 2. 모델 구조

Qwen 2.5는 Dense Transformer를 사용하는 모델들과 MoE Transformer를 사용하는 Qwen2.5-Turbo, Qwen2.5-Plus 모델로 나뉩니다. Qwen2.5-Turbo와 Qwen2.5-Plus 모델은 Qwen 팀에서 제공하는 독점 모델로, 별도의 모델 구조나 weight 파일이 공개되지 않았습니다.

Dense Transformer를 사용하는 모델은 작은 규모(0.5B)부터 큰 규모(72B)까지 다양한 크기로 공개되었습니다. 이 중 Qwen 2.5-3B와 Qwen 2.5-72B를 제외하고 Apache 2.0 라이센스에 해당하는 나머지 모델들은 상업적으로도 사용이 가능합니다.

![](/assets/img/llm/qwen_models.png)

### 2.1 MoE Transformer

![](/assets/img/llm/MoE.png)

_Switch Transformer_

[Fedus et al. (2021)](https://arxiv.org/pdf/2101.03961)은 **Mixture of Experts(MoE)** 개념을 Transformer 구조에 적용하여 LLM을 효율적으로 확장하는 방법을 제안했습니다. 여기에서 제안된 모델이 Transformer의 **feed-forward network(FFN)를 MoE layer로 교체**하여 만든 Switch Transformer입니다. 이 MoE layer는여러 개의 전문가(expert)와 각 토큰 $x$에 가장 적합하다고 판단되는 1개의 전문가를 선택하여 해당 토큰을 할당하는 라우팅 메커니즘으로 구성됩니다. 위 그림은 4개의 전문가가 존재하는 케이스를 표현한 것입니다. 이렇게 하면, 모델은 모든 매개변수를 항상 사용하지 않고, 필요한 부분만 활성화하여 계산 비용을 절감하면서도 강력한 성능을 유지할 수 있습니다.

Qwen2.5-Turbo, Qwen2.5-Plus 모델도 이러한 MoE Transformer를 사용했습니다. 하지만 Switch Transformer처럼 1개의 전문가만 사용하는 것이 아니라, **2~4개의 전문가를 선택하여 더 넓은 범위의 지식을 활용**하면서도 계산 비용을 효율적으로 유지합니다. Dense Transformer는 MoE layer를 사용하지 않고, 보편적으로 사용되는 decoder-only Transformer를 사용한 모델을 가리킵니다.

### 2.2 Post-Training

#### Supervised Fine-Tuning (SFT)

Qwen 2.5는 Post-training의 첫 단계로 **100만 개 이상의 샘플을 사용한 SFT**를 수행합니다. 이 단계는 모델이 다양한 지시사항을 정확하게 이해하고 따르도록 학습시킵니다. 특히, Agent가 생성한 대규모 지시 데이터셋을 활용하여 긴 지시사항에 대한 데이터 부족 문제를 해결합니다.

#### Reinforcement Learning (RL)

SFT 이후에는 다단계 강화학습을 적용하여 모델이 사용자의 지시를 더 잘 이해하고, 사람의 선호도에 맞는 답변을 생성하도록 했습니다. 이 과정은 다음과 같은 두 가지 주요 기술을 포함합니다.

- **DPO (Direct Preference Optimization)**: 오프라인 학습 기술로, 모델이 인간의 선호도(human preference)에 부합하는 응답을 생성하도록 직접적으로 최적화합니다.
- **GRPO (Group Relative Policy Optimization)**: 온라인 학습 기술로, 모델이 여러 답변 중 인간이 선호하는 답변을 선택하도록 학습시킵니다. 이는 모델의 판단 능력을 개선하고, 다양한 상황에서 더 적절한 응답을 생성하는 데 도움을 줍니다.

Llama 시리즈와 Qwen의 모델 정보를 비교해 보면 아래와 같습니다.

| 구분                                  | **Llama 3**          | **Llama 3.1**        | **Qwen 2.5 (Dense)**  | **Qwen 2.5 (MoE)**    |
| ------------------------------------- | -------------------- | -------------------- | --------------------- | --------------------- |
| **모델 구조**                         | Dense Transformer    | Dense Transformer    | Dense Transformer     | MoE Transformer       |
| **Attention**                         | GQA                  | GQA                  | GQA                   | GQA                   |
| **Activation Function**               | SwiGLU               | SwiGLU               | SwiGLU                | SwiGLU                |
| **Positional Embedding**              | RoPE                 | RoPE                 | RoPE                  | RoPE                  |
| **정규화**                            | RMSNorm              | RMSNorm              | RMSNorm               | RMSNorm               |
| **pre-training 데이터셋 규모 (토큰)** | 15조                 | 15조                 | 18조                  | 18조                  |
| **모델 크기**                         | 8B, 70B              | 8B, 70B, 405B        | 0.5B ~ 72B            | Turbo, Plus, Max      |
| **Context Window**                    | 8K                   | 128K                 | 128K                  | 1M                    |
| **Post-Training**                     | SFT → RLHF (PPO+DPO) | SFT → RLHF (PPO+DPO) | SFT → RLHF (GRPO+DPO) | SFT → RLHF (GRPO+DPO) |

## 3. 모델 성능

![](/assets/img/llm/qwen_performance.png)

## Reference

- [LLM 아키텍처에 Mixture of Experts(MoE)를 활용하기](https://developer.nvidia.com/ko-kr/blog/applying-mixture-of-experts-in-llm-architectures/)

- [Mixture-of-Experts with Expert Choice Routing](https://research.google/blog/mixture-of-experts-with-expert-choice-routing/)
- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity