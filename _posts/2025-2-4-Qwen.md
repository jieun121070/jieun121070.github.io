---
title: "[Paper Review] Qwen 2.5"
date: 2025-2-4
author: jieun
math: True
categories: [Language-Model]
tags: [LLM, Qwen]
typora-root-url: ..
---

Qwen은 알리바바 클라우드에서 개발한 LLM으로, 코딩과 수학적 추론 능력이 뛰어난 모델입니다.

![](/assets/img/llm/qwen1.png)

## 1. 모델 구조

### Dense Transformer

### Dense Transformer + MoE

![](/assets/img/llm/MoE.png)



Llama 시리즈와 Qwen의 모델 정보를 비교해 보면 아래와 같습니다.

| 구분                     | **Llama 3**          | **Llama 3.1**        | **Qwen 2.5 (Dense Transformer)** | **Qwen 2.5 (MoE Transformer)** |
| ------------------------ | -------------------- | -------------------- | -------------------------------- | ------------------------------ |
| **모델 구조**            | Dense Transformer    | Dense Transformer    | Dense Transformer                | MoE Transformer                |
| **Attention**            | GQA                  | GQA                  | GQA                              | GQA                            |
| **Activation Function**  | SwiGLU               | SwiGLU               | SwiGLU                           | SwiGLU                         |
| **Positional Embedding** | RoPE                 | RoPE                 | RoPE                             | RoPE                           |
| **정규화**               | RMSNorm              | RMSNorm              | RMSNorm                          | RMSNorm                        |
| **학습 데이터셋 규모**   | 15조 토큰            | 15조 토큰            | 18조 토큰                        | 데이터셋 규모 정보 없음        |
| **모델 크기**            | 8B, 70B              | 8B, 70B, 405B        | 0.5B ~ 72B                       | Turbo, Plus, Max               |
| **Context Window**       | 8K                   | 128K                 | 128K                             | 1M                             |
| **Post-Training**        | SFT → RLHF (PPO+DPO) | SFT → RLHF (PPO+DPO) | SFT → RLHF (GRPO+DPO)            | SFT → RLHF (GRPO+DPO)          |

## 2. 모델 성능

## Reference

- [LLM 아키텍처에 Mixture of Experts(MoE)를 활용하기](https://developer.nvidia.com/ko-kr/blog/applying-mixture-of-experts-in-llm-architectures/)

- [Mixture-of-Experts with Expert Choice Routing](https://research.google/blog/mixture-of-experts-with-expert-choice-routing/)
- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity