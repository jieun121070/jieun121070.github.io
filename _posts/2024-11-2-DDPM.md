---
title: "[Paper Review] DDPM: Denoising Diffusion Probabilistic Models"
date: 2024-11-2
author: jieun
math: True
categories: [Vision]
tags: [DDPM]
typora-root-url: ..
---

[Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)은 diffusion 확률 모델을 이용해 고품질의 이미지를 생성하는 모델입니다. DDPM의 핵심 아이디어는 timestep $t$에 따라 데이터에 노이즈를 점차 확산(diffusion)시킨 다음, 그 과정을 거꾸로 학습해서 데이터를 생성하는 것입니다.

## 1. Markov Chain

모델 구조에 대해 자세히 살펴보기 전에, 먼저 Markov Chain에 대해 알아보겠습니다. **Markov Property**를 만족하는 시퀀스를 **Markov Chain**이라고 합니다. Markov Property는 과거와 현재 상태가 주어졌을 때, **미래 상태 $X_t$의 조건부 확률 분포가** 과거 상태들로부터 독립적으로 **현재 상태 $X_{t-1}$에 의해서만 결정된다**는 것을 뜻합니다.

$$\Pr\!\bigl(X_t = x_t \,\big|\, X_0 = x_0,\dots,X_{t-1} = x_{t-1}\bigr)
\;=\;
\Pr\!\bigl(X_t = x_t \,\big|\, X_{t-1} = x_{t-1}\bigr)
,\quad\forall\,t\ge 1.$$

뒤에서 자세히 설명할 예정이지만, DDPM은 원본 이미지 $x_0$에 노이즈를 순차적으로 누적해서 더합니다. $x_{t-1}$에 노이즈를 더해 $x_t$를 만드는 과정을 반복하는 것입니다. 따라서 시퀀스 $x_0,..., x_T$는 Markov Chain이 됩니다.

$$x_t=\sqrt{1-\beta_t}\,x_{t-1}+\sqrt{\beta_t}\,\varepsilon,\;\varepsilon \sim \mathcal N(\mathbf 0,\mathbf I).$$

노이즈로는 **가우시안 노이즈**가 사용됩니다. 저자들이 가우시안 노이즈를 선택한 이유는 계산 편의성 때문입니다. 조건부 관점에서 $x_{t-1}$는 상수 취급되고, $\varepsilon$는 표준 가우시안 벡터입니다. 가우시안은 선형 변환 후에도 가우시안이므로, $\varepsilon \sim \mathcal N(\mathbf 0,\mathbf I)$에 $\sqrt{\beta_t}$를 곱해도 가우시안이 유지됩니다. 따라서 조건부 분포 $q(x_t \mid x_{t-1})$도 평균이 $\sqrt{1-\beta_t}\,x_{t-1}$이고, 공분산이 $\beta_t \mathbf I$인 가우시안 분포를 따르게 되는 것입니다. 이러한 성질을 닫힘 성질이라고 합니다.

$$q\!\bigl(x_t \,\big|\, x_{t-1}\bigr) = \mathcal N\!\Bigl(\sqrt{1-\beta_t}\,x_{t-1},\;\beta_t \mathbf I \Bigr)$$

## 2. 모델 학습

모델 학습 과정을 정리해보면 아래와 같습니다.

- `Step 1` timestep $t \sim \text{Uniform}\{1,\dots,T\}$를 뽑고, 원본 이미지 $x_0$에 $t$시점까지 노이즈를 누적해서 더한 이미지 $x_t$를 구합니다. 앞서 설명한 닫힘 성질 덕분에 원본 이미지 $x_0$에 매 시점 노이즈를 누적해서 더하는 과정을 거치지 않고,  $x_t$를 한 번에 구할 수 있습니다. $\beta$-스케줄은 일반적으로 선형(1e-4 → 0.02) 또는 코사인 스케줄을 많이 사용합니다. 작은 $\beta$부터 시작해 서서히 노이즈를 키워야 역방향 학습이 안정적으로 이루어집니다.

$$x_t=\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\varepsilon,
\qquad \varepsilon \sim \mathcal N(\mathbf 0,\mathbf I),\;\bar{\alpha}_t = \prod_{s=1}^{t} (1 - \beta_s)$$

- `Step 2` **UNet**에 $x_t$와 시점 $t$를 입력해 $x_t$에 더해진 노이즈 $\varepsilon$를 예측합니다. 이 때, $t$는 sinusoidal position embedding 후에 MLP를 거친 다음 각 ResNetBlock에 더해집니다. 아래 그림에서 실선 화살표가 UNet($p_\theta$) 추정 경로입니다.

$$\hat\varepsilon_\theta(x_t,t)$$

![](/assets/img/diffusion/ddpm.png)

- `Step 3` MSE Loss를 계산하고 Adam optimizer로 파라미터를 업데이트합니다. 

$$
\mathcal L_{\text{simple}}
   = \mathbb E_{t,\,x_0,\,\varepsilon}
     \bigl[
        \,\bigl\|\,
           \varepsilon \;-\;
           \hat\varepsilon_{\theta}(x_t,\,t)
        \bigr\|_2^2
     \bigr]
$$

## Reference

- [확률편미분방정식과 인공지능](https://horizon.kias.re.kr/25133/)