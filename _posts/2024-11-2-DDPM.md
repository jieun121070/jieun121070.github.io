---
title: "[Paper Review] DDPM: Denoising Diffusion Probabilistic Models"
date: 2024-11-2
author: jieun
math: True
categories: [Vision]
tags: [DDPM]
typora-root-url: ..
---

[Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)은 diffusion 확률 모델을 이용해 고품질의 이미지를 생성하는 모델입니다. DDPM의 핵심 아이디어는 timestep $t$에 따라 데이터에 노이즈를 점차 확산(diffusion)시킨 다음, 그 과정을 거꾸로 학습해서 데이터를 생성하는 것입니다.

## 1. Markov Chain

모델 구조에 대해 자세히 살펴보기 전에, 먼저 Markov Chain에 대해 알아보겠습니다. **Markov Property**를 만족하는 시퀀스를 **Markov Chain**이라고 합니다. Markov Property는 과거와 현재 상태가 주어졌을 때, **미래 상태 $X_t$의 조건부 확률 분포가** 과거 상태들로부터 독립적으로 **현재 상태 $X_{t-1}$에 의해서만 결정된다**는 것을 뜻합니다.

$$\Pr\!\bigl(X_t = x_t \,\big|\, X_0 = x_0,\dots,X_{t-1} = x_{t-1}\bigr)
\;=\;
\Pr\!\bigl(X_t = x_t \,\big|\, X_{t-1} = x_{t-1}\bigr)
,\quad\forall\,t\ge 1.$$

뒤에서 자세히 설명할 예정이지만, DDPM은 원본 이미지 $x_0$에 노이즈를 순차적으로 누적해서 더합니다. $x_{t-1}$에 노이즈를 더해 $x_t$를 만드는 과정을 반복하는 것입니다. 따라서 시퀀스 $x_0,..., x_T$는 Markov Chain이 됩니다.

$$x_t=\sqrt{1-\beta_t}\,x_{t-1}+\sqrt{\beta_t}\,\varepsilon,\;\varepsilon \sim \mathcal N(\mathbf 0,\mathbf I).$$

노이즈로는 **가우시안 노이즈**가 사용됩니다. 저자들이 가우시안 노이즈를 선택한 이유는 계산 편의성 때문입니다. 조건부 관점에서 $x_{t-1}$는 상수 취급되고, $\varepsilon$는 표준 가우시안 벡터입니다. 가우시안은 선형 변환 후에도 가우시안이므로, $\varepsilon \sim \mathcal N(\mathbf 0,\mathbf I)$에 $\sqrt{\beta_t}$를 곱해도 가우시안이 유지됩니다. 따라서 조건부 분포 $q(x_t \mid x_{t-1})$도 평균이 $\sqrt{1-\beta_t}\,x_{t-1}$이고, 공분산이 $\beta_t \mathbf I$인 가우시안 분포를 따르게 되는 것입니다. 이러한 성질을 닫힘 성질이라고 합니다.

$$q\!\bigl(x_t \,\big|\, x_{t-1}\bigr) = \mathcal N\!\Bigl(\sqrt{1-\beta_t}\,x_{t-1},\;\beta_t \mathbf I \Bigr)$$

## 2. 모델 학습과 이미지 생성

### 모델 학습

모델 학습 과정을 정리해보면 아래와 같습니다.

- `Step 1` timestep $t \sim \text{Uniform} \{\ 1,\dots,T \}\ $를 뽑고, 원본 이미지 $x_0$에 $t$시점까지 정방향으로 노이즈를 누적해서 더한 이미지 $x_t$를 구합니다. 앞서 설명한 닫힘 성질 덕분에 원본 이미지 $x_0$에 매 시점 노이즈를 누적해서 더하는 과정을 거치지 않고,  $x_t$를 한 번에 샘플링할 수 있습니다. $\beta$-스케줄은 일반적으로 선형(1e-4 → 0.02) 또는 코사인 스케줄을 많이 사용합니다. 작은 $\beta$부터 시작해 서서히 노이즈를 키워야 역방향 학습이 안정적으로 이루어집니다.

$$x_t=\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\varepsilon,
\qquad \varepsilon \sim \mathcal N(\mathbf 0,\mathbf I),\;\bar{\alpha}_t = \prod_{s=1}^{t} (1 - \beta_s)$$

- `Step 2` **U-Net**에 $x_t$와 시점 $t$를 입력해 $x_t$에 더해진 노이즈 $\varepsilon$를 예측합니다. 이 때, $t$는 sinusoidal position embedding 후에 MLP를 거친 다음 ResidualBlock에 FiLM 방식으로 주입합니다. 아래 그림에서 실선 화살표가 UNet($p_\theta$) 추정 경로입니다.

$$\hat\varepsilon_\theta(x_t,t)$$

![](/assets/img/diffusion/ddpm.png)

- `Step 3` $\varepsilon$-MSE Loss를 계산하고 Adam optimizer로 파라미터를 업데이트합니다. $\varepsilon$-MSE Loss는 timestep별 $\sigma^2$ 가중 Denoising Score Matching(DSM) loss와 동치입니다. 따라서 둘 중 어떤 loss를 사용하든 같은 $\theta$에 수렴하게 됩니다. 다시 말해, **$\varepsilon$-MSE를 최소화하면 score를 정확하게 예측할 수 있게 됩니다.**

$$\mathcal L_{\text{simple}}   = \mathbb E_{t,\,x_0,\,\varepsilon}     \bigl[        \,\bigl\|\,           \varepsilon -           \hat\varepsilon_{\theta}(x_t,\,t)        \bigr\|_2^2     \bigr]$$

`Step 3`에서 설명한 score는 아래와 같이 정의됩니다. 어떤 시점 $t$ (또는 노이즈 $\sigma_t$)에서의 확률 밀도 함수 $p_t(x_t)$가 주어졌을 때, 그 log-likelihood를 데이터 $x_t$에 대해 편미분한 벡터입니다. 이는 log-likelihood를 빠르게 높이는 방향을 의미합니다.

$$s(x)=\nabla_{x}\,\log p(x)$$

이미지 생성 모델 관점에서 확률 밀도 함수 $p_t(x_t)$의 log-likelihood는 모델이 생성한 이미지 $x_t$가 얼마나 있을 법한 이미지인지를 나타낸다고 볼 수 있습니다. score는 모델이 자연스러운 이미지를 만들어내기 위한 방향입니다. 가우시안의 score 공식은 아래와 같습니다.

$$s(x)=\nabla_{x}\,\log \mathcal N\!\bigl(x;\,\mu,\Sigma\bigr)
   \;=\;
   -\,\Sigma^{-1}\,\bigl(x-\mu\bigr)$$

특히 DDPM처럼 공분산 행렬이 스칼라와 단위 행렬 $I$의 곱으로 표현되는 등방 가우시안($\Sigma=\sigma^2I$)인 경우, 아래와 같은 관계가 성립합니다. 따라서 모델이 $\varepsilon$를 잘 맞추면 score 역시 간단한 연산으로 얻을 수 있습니다.

$$s(x)=-\frac{x-\mu}{\sigma^2}=-\frac{\varepsilon}{\sigma}$$

### 이미지 생성

- `Step 1` $x_T$를 초기화합니다. $x_T \sim \mathcal N(\mathbf 0,\mathbf I)$
- `Step 2` **U-Net**으로 $\varepsilon$를 예측합니다. 필요 시, score로 변환합니다. $\varepsilon$와 score는 아래와 같은 선형 관계입니다.

$$\hat{s}=−\hat{\varepsilon}/\sigma_{t}$$

- `Step 3` 매 역방향 step $x_t \rightarrow x_{t-1}$에서 $x_{t-1}$을 계산합니다. 이 때, $x_t$가 score 방향으로 조금씩 수정되고, 그 누적 결과로 노이즈 $x_T$가 데이터 $x_0$로 복원됩니다. 

$$
x_{t-1} =
\frac{1}{\sqrt{1-\beta_t}}\,
\Bigl(
    x_t - \frac{\beta_t}{\sqrt{\bar\alpha_t}}\,
    \hat\varepsilon_\theta(x_t,\,t)
\Bigr)
+ \sqrt{\tilde\beta_t}\,z,
\qquad
z \sim \mathcal N(0,I),
\;
\tilde\beta_t =
\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\,\beta_t.
$$

## 3. 모델 구조

![](/assets/img/diffusion/unet.png)

_U-Net architecture_

앞서 설명한 바와 같이 DDPM은 노이즈 $\varepsilon$를 예측할 때, 그리고 이미지를 생성할 때 **U-Net**을 사용합니다. [이곳](https://nn.labml.ai/diffusion/ddpm/unet.html)에서 코드를 확인할 수 있습니다. 

## Reference

- [확률편미분방정식과 인공지능](https://horizon.kias.re.kr/25133/)