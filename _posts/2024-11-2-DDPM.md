---
title: "[Paper Review] DDPM: Denoising Diffusion Probabilistic Models (1)"
date: 2024-11-2
author: jieun
math: True
categories: [Vision]
tags: [DDPM, VAE, GAN]
typora-root-url: ..
---

생성 모델은 데이터의 분포를 학습하여 그와 유사한 새로운 데이터를 생성하는 모델입니다. Vision 분야의 생성 모델로는 [VAE](https://jieun121070.github.io/posts/Variational-Autoencoder(VAE)/)와 [GAN](https://jieun121070.github.io/posts/Generative-Adversarial-Networks/)에 대해 다룬 바 있는데요. **GAN**은 고품질의 이미지를 생성할 수 있지만, adversarial learning 방식을 따르기 때문에 학습이 불안정한 편입니다. **VAE**는 GAN과 달리 likelihood 기반 생성 모델입니다. likelihood $p(x) = \int p(x \vert z)p(z)dz$는 현실적으로 계산하기 어려워서 variational inference를 사용해 이를 근사하는 ELBO를 최대화합니다. 아래 식에서 $p(x \vert z)$는 decoder가 학습하는 분포이고, 근사 사후 분포 $q(z \vert x)$는 encoder가 학습하는 분포입니다.

$$\log p(x) \ge \mathbb{E}_{q(z \mid x)} [\log p(x \mid z)] − D_{KL}(q(z \mid x) \mid\mid p(z))$$

VAE는 likelihood를 추론할 수 있고 수렴이 안정적이라는 장점이 있지만, GAN보다 상대적으로 흐릿한 이미지를 생성할 가능성이 높습니다. **Normalizing Flows**는 VAE와 같은 likelihood 기반 생성 모델로, 정규 분포같은 단순한 분포로부터 복잡한 데이터 분포로의 변환을 학습해서 이미지를 생성합니다. 역변환 가능한 구조 덕분에 정확하게 likelihood를 계산할 수 있습니다.

likelihood를 추론하는 VAE나 likelihood를 정확하게 계산해내는 Normalizing Flows 같은 **likelihood 기반 생성 모델은 모델이 얼마나 자연스러운 이미지를 생성했는지를 정량적으로 평가할 수 있습니다.** GAN은 어느 정도로 그럴듯한 이미지를 생성했는지 확률로 표현할 수 없습니다. 또한 GAN보다 학습이 안정적이라는 장점이 있습니다.

![](/assets/img/diffusion/GANs_Diffusion_Autoencoders.png)

이번 포스트에서 소개할 [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239) (DDPM)은 diffusion process에 기반한 이미지 생성 모델입니다. DDPM의 핵심 아이디어는 timestep $t$에 따라 데이터에 노이즈를 점진적으로 확산(diffusion)시킨 다음, 그 과정을 거꾸로 학습하는 것입니다. likelihood 기반 최적화를 통해 학습 안정성을 높이면서도 고품질의 이미지를 생성할 수 있습니다.

## 1. Markov Chain

DDPM에 대해 자세히 살펴보기 전에, 먼저 Markov Chain에 대해 알아보겠습니다. **Markov Property**를 만족하는 시퀀스를 **Markov Chain**이라고 합니다. Markov Property는 과거와 현재 상태가 주어졌을 때, **미래 상태 $X_t$의 조건부 확률 분포가** 과거 상태들로부터 독립적으로 **현재 상태 $X_{t-1}$에 의해서만 결정된다**는 것을 뜻합니다.

$$\Pr\!\bigl(X_t = x_t \,\big|\, X_0 = x_0,\dots,X_{t-1} = x_{t-1}\bigr)
\;=\;
\Pr\!\bigl(X_t = x_t \,\big|\, X_{t-1} = x_{t-1}\bigr)
,\quad\forall\,t\ge 1.$$

뒤에서 자세히 설명할 예정이지만, DDPM은 원본 이미지 $x_0$에 노이즈를 순차적으로 누적해서 더합니다. $x_{t-1}$에 노이즈를 더해 $x_t$를 만드는 과정을 반복하는 것입니다. 따라서 시퀀스 $x_0,..., x_T$는 Markov Chain이 됩니다.

$$x_t=\sqrt{1-\beta_t}\,x_{t-1}+\sqrt{\beta_t}\,\varepsilon,\;\varepsilon \sim \mathcal N(\mathbf 0,\mathbf I).$$

노이즈로는 **가우시안 노이즈**가 사용됩니다. 저자들이 가우시안 노이즈를 선택한 이유는 계산 편의성 때문입니다. 조건부 관점에서 $x_{t-1}$는 상수 취급되고, $\varepsilon$는 표준 가우시안 벡터입니다. 가우시안은 선형 변환 후에도 가우시안이므로, $\varepsilon \sim \mathcal N(\mathbf 0,\mathbf I)$에 $\sqrt{\beta_t}$를 곱해도 가우시안이 유지됩니다. 따라서 조건부 분포 $q(x_t \mid x_{t-1})$도 평균이 $\sqrt{1-\beta_t}\,x_{t-1}$이고, 공분산이 $\beta_t \mathbf I$인 가우시안 분포를 따르게 되는 것입니다. 이러한 성질을 닫힘 성질이라고 합니다.

$$q\!\bigl(x_t \,\big|\, x_{t-1}\bigr) = \mathcal N\!\Bigl(\sqrt{1-\beta_t}\,x_{t-1},\;\beta_t \mathbf I \Bigr)$$

## 2. DDPM 개요

![](/assets/img/diffusion/ddpm.png)



## Reference

- [확률편미분방정식과 인공지능](https://horizon.kias.re.kr/25133/)
- [Improving Diffusion Models as an Alternative To GANs](https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/)
