---
title: "분포 간의 거리를 측정하는 방법들"
date: 2023-7-3
author: jieun
math: True
categories: [Data-Analysis]
tags: [KL-Divergence, JS-Divergence, Wasserstein, GAN]
typora-root-url: ..
---

## Kullback-Leiver(KL) Divergence

- 두 분포 $p$와 $q$가 주어졌을 때, $p$가 $q$에 대해 얼마나 다른지 측정

  - $p$가 $q$의 정보량을 얼마나 잘 보존하는지 측정
  - 정보량을 잘 보존할수록 서로 비슷한 분포

- 상대적인 엔트로피(relative entropy)

  $$D_{KL}=E[- \log q(x)]-E[-\log p(x)]$$

  $$D_{KL}(p \lvert\rvert q)= \int_xp(x) \log \frac{p(x)}{q(x)}dx$$

  - 확률 분포 $p$와 확률 분포 $q$가 모든 point에서 같을 때 최솟값 0을 가짐

- KL divergence는 비대칭(asymmetric)

  $$D_{KL}(p \lvert\rvert q) \ne D_{KL}(q \lvert\rvert p)$$

  - 확률 분포 $p$가 0에 가깝고, 확률 분포 $q$는 0이 아닐 때 $q$ 효과는 무시됨
  - 동등하게 중요한 두 분포 사이의 유사도를 측정하고 싶을 때 적합하지 않음

## Jensen-Shannon(JS) Divergence

- 두 분포 $p$과 $q$가 주어졌을 때, 두 분포의 중간 지점과의 차이를 측정

  $$D_{JS}(p \lvert\rvert q)=\frac{1}{2}D_{KL}(p \lvert\rvert \frac{p+q}{2})+\frac{1}{2}D_{KL}(q \lvert\rvert \frac{p+q}{2})$$

- KL divergence와 JS divergence 비교
  ![](/assets/img/gan/kl.png)
  _KL divergence와 JS divergence 비교_

  - [0, 1] 범위로 한정됨
  - KL divergence보다 부드러움
  - KL divergence와 달리 JS divergence는 대칭(symmetric)

- GAN의 loss function을 최적화하는 것은 $D_{JS}$를 최적화하는 것과 같음

  $$\underset{G}{\min}\,\underset{D}{\max}\,V(D, G)=E_{x \sim p_{data}(x)}[logD(x)]+E_{z \sim p_{z}(z)}[log(1-D(G(z)))]$$

  - 최적의 판별기는 실제 분포 $p_{data}(x)$와 가짜 분포 $p_z(z)$의 거리 $D_{JS}$를 최대화
  - 최적의 생성기는 실제 분포 $p_{data}(x)$와 가짜 분포 $p_z(z)$의 거리 $D_{JS}$를 최소화 ($p_{data}(x)=p_z(z)$)

  $$D^\ast(x) = \frac{p_{data}}{p_{data}+p_z}= \frac{1}{2}$$

  - [Huszar, 2015](https://arxiv.org/pdf/1511.05101.pdf)에서는 GAN이 성공한 요인 중 하나가 loss function을 KL divergence에서 JS divergence로 바꾸었기 때문이라고 주장하기도 함

## Wasserstein 1 (EMD)

- loss function을 JS divergence로 대체했음에도 GAN을 학습시키는 것은 여전히 어려운 문제
  - 판별기는 $D_{JS}$를 최대화하고자 하며 생성기는 $D_{JS}$를 최소화하고자 하므로 수렴하기 어려움
- Wasserstein 거리는 두 분포 $p$, $q$ 사이의 최소 이동 비용을 측정

$$W_p(P,Q) = \left( \inf_{\gamma \in \Pi(P,Q)} \int_{\mathbb{R}^d \times \mathbb{R}^d} \|x - y\|^p \, d\gamma(x,y) \right)^{\frac{1}{p}}$$

## Reference

- https://www.youtube.com/watch?v=FGP20ciUxlo
- [From GAN to WGAN](https://lilianweng.github.io/posts/2017-08-20-gan/)
